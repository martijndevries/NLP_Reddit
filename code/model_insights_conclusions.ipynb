{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42c59690-302e-43f9-86f9-1646fb13fde6",
   "metadata": {},
   "source": [
    "# Model Insights and Conclusions\n",
    "\n",
    "This is notebook 5 (out of 5) for <b>Project 3</b> of the GA Data Science Immersive<br>\n",
    "Notebook by: <b>Martijn de Vries</b><br>\n",
    "martijndevries91@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a829e131-dd87-4bcf-b277-1172433d618c",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "A US political consultancy company is researching how news sources and discussed topics differ between the US political mainstream and the conservative right-wing media. In the last decade or so, the US political right-wing has been increasingly described as living in an entirely separate information ecosystem from the political mainstream. In order to gauge how intense this effect is, we will collect, process, and classify the Reddit content of two politically-themed subreddit that reflect the mainstream and conservative voters respectively: <b>r/politics</b> and <b>r/conservative</b>. \n",
    "\n",
    "For this project, we will build two separate branches of models: one for post submissions (largely consisting of links to news sites), and another for comments (consisting of actual Reddit users discussing political news). As this is a binary classification problem where the two classes are of equal interest and will be approximately balanced, we will use the accuracy score as the main metric to gauge the success of the classification model. \n",
    "\n",
    "Because political news is always evolving, we have chosen a specific moment in time: the month leading up to the 2022 midterms, October 6th to November 6th 2022. This ensures that 1) the same news cycle is covered for both subreddits, 2) both subreddits were at peak activity, and 3) maximum potential for interesting insights in the way that news is discussed within these two subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af998e-d6b4-466d-854e-d1c444050c42",
   "metadata": {},
   "source": [
    "## In this Notebook\n",
    "\n",
    "I will load in the best models from the modeling notebook that I pickled. I will do some interpretation regarding which features are the most predictive, and look at the final accuracy score vs the baseline. For the comments model, I will do some additional analysis to try and see why comments get misclassified. At the end, I give a summary and conclusions for the entire project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8de25501-3ff6-4d02-a416-eaafa28a059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "#custom\n",
    "from custom_funcs import Tfidf_BigramReducer, my_preprocessor\n",
    "\n",
    "#sklearn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2383a1-7bc9-48be-8367-1c0dc8f9b011",
   "metadata": {},
   "source": [
    "## 1) Title model: insights\n",
    "\n",
    "Let's start by re-loading the data, and the stacked model that I pickled in the modeling notebook. \n",
    "\n",
    "Sidenote: I put the custom transformer class Tfidf_BigramReducer and the my_preprocessor function in a .py file so that I can just import them (see imports above) without having to copy-paste them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1eae3f0-0750-4de6-934c-bce228f2d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = pd.read_csv('../data/all_submissions_cleaned.csv')\n",
    "\n",
    "post_df['subreddit'] = post_df['subreddit'].map({'conservative':0, 'politics':1})\n",
    "\n",
    "X = post_df[['title', 'num_comments', 'domain']]\n",
    "y = post_df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6173067a-624f-454d-931b-041343346af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickled_models/stacked_model_submissions.pkl', 'rb') as pickle_in:\n",
    "    stacked_model_subs = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d3670e-961c-45d8-80ee-e506d9f52f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(estimators=[(&#x27;m1&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;ct&#x27;,\n",
       "                                                 ColumnTransformer(transformers=[(&#x27;tfbg&#x27;,\n",
       "                                                                                  Tfidf_BigramReducer(bf=0.1,\n",
       "                                                                                                      max_features=30000,\n",
       "                                                                                                      preprocessor=&lt;function my_preprocessor at 0x7fafdb2c93a0&gt;,\n",
       "                                                                                                      stop_words=&#x27;english&#x27;),\n",
       "                                                                                  &#x27;title&#x27;),\n",
       "                                                                                 (&#x27;ss&#x27;,\n",
       "                                                                                  StandardScaler(),\n",
       "                                                                                  [&#x27;num_comments&#x27;]),\n",
       "                                                                                 (&#x27;ohe&#x27;,\n",
       "                                                                                  OneHotEncoder(drop=&#x27;first&#x27;,\n",
       "                                                                                                sparse_output=False),\n",
       "                                                                                  [&#x27;domain&#x27;])])),\n",
       "                                                (&#x27;lr&#x27;,\n",
       "                                                 Lo...\n",
       "                                Pipeline(steps=[(&#x27;ct&#x27;,\n",
       "                                                 ColumnTransformer(transformers=[(&#x27;tfbg&#x27;,\n",
       "                                                                                  Tfidf_BigramReducer(bf=0.1,\n",
       "                                                                                                      max_features=40000,\n",
       "                                                                                                      preprocessor=&lt;function my_preprocessor at 0x7fafdb2c93a0&gt;,\n",
       "                                                                                                      stop_words=&#x27;english&#x27;),\n",
       "                                                                                  &#x27;title&#x27;),\n",
       "                                                                                 (&#x27;mm&#x27;,\n",
       "                                                                                  MinMaxScaler(),\n",
       "                                                                                  [&#x27;num_comments&#x27;]),\n",
       "                                                                                 (&#x27;ohe&#x27;,\n",
       "                                                                                  OneHotEncoder(drop=&#x27;first&#x27;,\n",
       "                                                                                                sparse_output=False),\n",
       "                                                                                  [&#x27;domain&#x27;])])),\n",
       "                                                (&#x27;nb&#x27;, MultinomialNB())]))],\n",
       "                   final_estimator=LogisticRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(estimators=[(&#x27;m1&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;ct&#x27;,\n",
       "                                                 ColumnTransformer(transformers=[(&#x27;tfbg&#x27;,\n",
       "                                                                                  Tfidf_BigramReducer(bf=0.1,\n",
       "                                                                                                      max_features=30000,\n",
       "                                                                                                      preprocessor=&lt;function my_preprocessor at 0x7fafdb2c93a0&gt;,\n",
       "                                                                                                      stop_words=&#x27;english&#x27;),\n",
       "                                                                                  &#x27;title&#x27;),\n",
       "                                                                                 (&#x27;ss&#x27;,\n",
       "                                                                                  StandardScaler(),\n",
       "                                                                                  [&#x27;num_comments&#x27;]),\n",
       "                                                                                 (&#x27;ohe&#x27;,\n",
       "                                                                                  OneHotEncoder(drop=&#x27;first&#x27;,\n",
       "                                                                                                sparse_output=False),\n",
       "                                                                                  [&#x27;domain&#x27;])])),\n",
       "                                                (&#x27;lr&#x27;,\n",
       "                                                 Lo...\n",
       "                                Pipeline(steps=[(&#x27;ct&#x27;,\n",
       "                                                 ColumnTransformer(transformers=[(&#x27;tfbg&#x27;,\n",
       "                                                                                  Tfidf_BigramReducer(bf=0.1,\n",
       "                                                                                                      max_features=40000,\n",
       "                                                                                                      preprocessor=&lt;function my_preprocessor at 0x7fafdb2c93a0&gt;,\n",
       "                                                                                                      stop_words=&#x27;english&#x27;),\n",
       "                                                                                  &#x27;title&#x27;),\n",
       "                                                                                 (&#x27;mm&#x27;,\n",
       "                                                                                  MinMaxScaler(),\n",
       "                                                                                  [&#x27;num_comments&#x27;]),\n",
       "                                                                                 (&#x27;ohe&#x27;,\n",
       "                                                                                  OneHotEncoder(drop=&#x27;first&#x27;,\n",
       "                                                                                                sparse_output=False),\n",
       "                                                                                  [&#x27;domain&#x27;])])),\n",
       "                                                (&#x27;nb&#x27;, MultinomialNB())]))],\n",
       "                   final_estimator=LogisticRegression())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>m1</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ct: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;tfbg&#x27;,\n",
       "                                 Tfidf_BigramReducer(bf=0.1, max_features=30000,\n",
       "                                                     preprocessor=&lt;function my_preprocessor at 0x7fafdb2c93a0&gt;,\n",
       "                                                     stop_words=&#x27;english&#x27;),\n",
       "                                 &#x27;title&#x27;),\n",
       "                                (&#x27;ss&#x27;, StandardScaler(), [&#x27;num_comments&#x27;]),\n",
       "                                (&#x27;ohe&#x27;,\n",
       "                                 OneHotEncoder(drop=&#x27;first&#x27;,\n",
       "                                               sparse_output=False),\n",
       "                                 [&#x27;domain&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">tfbg</label><div class=\"sk-toggleable__content\"><pre>title</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Tfidf_BigramReducer</label><div class=\"sk-toggleable__content\"><pre>Tfidf_BigramReducer(bf=0.1, max_features=30000,\n",
       "                    preprocessor=&lt;function my_preprocessor at 0x7fafdb2c93a0&gt;,\n",
       "                    stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ss</label><div class=\"sk-toggleable__content\"><pre>[&#x27;num_comments&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ohe</label><div class=\"sk-toggleable__content\"><pre>[&#x27;domain&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(drop=&#x27;first&#x27;, sparse_output=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>m2</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ct: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;tfbg&#x27;,\n",
       "                                 Tfidf_BigramReducer(bf=0.1, max_features=25000,\n",
       "                                                     preprocessor=&lt;function my_preprocessor at 0x7fafdb2c93a0&gt;,\n",
       "                                                     stop_words=&#x27;english&#x27;),\n",
       "                                 &#x27;title&#x27;),\n",
       "                                (&#x27;ss&#x27;, StandardScaler(), [&#x27;num_comments&#x27;]),\n",
       "                                (&#x27;ohe&#x27;, OneHotEncoder(sparse_output=False),\n",
       "                                 [&#x27;domain&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">tfbg</label><div class=\"sk-toggleable__content\"><pre>title</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Tfidf_BigramReducer</label><div class=\"sk-toggleable__content\"><pre>Tfidf_BigramReducer(bf=0.1, max_features=25000,\n",
       "                    preprocessor=&lt;function my_preprocessor at 0x7fafdb2c93a0&gt;,\n",
       "                    stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ss</label><div class=\"sk-toggleable__content\"><pre>[&#x27;num_comments&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ohe</label><div class=\"sk-toggleable__content\"><pre>[&#x27;domain&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(sparse_output=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=175, random_state=123)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>m3</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ct: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;tfbg&#x27;,\n",
       "                                 Tfidf_BigramReducer(bf=0.1, max_features=40000,\n",
       "                                                     preprocessor=&lt;function my_preprocessor at 0x7fafdb2c93a0&gt;,\n",
       "                                                     stop_words=&#x27;english&#x27;),\n",
       "                                 &#x27;title&#x27;),\n",
       "                                (&#x27;mm&#x27;, MinMaxScaler(), [&#x27;num_comments&#x27;]),\n",
       "                                (&#x27;ohe&#x27;,\n",
       "                                 OneHotEncoder(drop=&#x27;first&#x27;,\n",
       "                                               sparse_output=False),\n",
       "                                 [&#x27;domain&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">tfbg</label><div class=\"sk-toggleable__content\"><pre>title</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Tfidf_BigramReducer</label><div class=\"sk-toggleable__content\"><pre>Tfidf_BigramReducer(bf=0.1, max_features=40000,\n",
       "                    preprocessor=&lt;function my_preprocessor at 0x7fafdb2c93a0&gt;,\n",
       "                    stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">mm</label><div class=\"sk-toggleable__content\"><pre>[&#x27;num_comments&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ohe</label><div class=\"sk-toggleable__content\"><pre>[&#x27;domain&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(drop=&#x27;first&#x27;, sparse_output=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingClassifier(estimators=[('m1',\n",
       "                                Pipeline(steps=[('ct',\n",
       "                                                 ColumnTransformer(transformers=[('tfbg',\n",
       "                                                                                  Tfidf_BigramReducer(bf=0.1,\n",
       "                                                                                                      max_features=30000,\n",
       "                                                                                                      preprocessor=<function my_preprocessor at 0x7fafdb2c93a0>,\n",
       "                                                                                                      stop_words='english'),\n",
       "                                                                                  'title'),\n",
       "                                                                                 ('ss',\n",
       "                                                                                  StandardScaler(),\n",
       "                                                                                  ['num_comments']),\n",
       "                                                                                 ('ohe',\n",
       "                                                                                  OneHotEncoder(drop='first',\n",
       "                                                                                                sparse_output=False),\n",
       "                                                                                  ['domain'])])),\n",
       "                                                ('lr',\n",
       "                                                 Lo...\n",
       "                                Pipeline(steps=[('ct',\n",
       "                                                 ColumnTransformer(transformers=[('tfbg',\n",
       "                                                                                  Tfidf_BigramReducer(bf=0.1,\n",
       "                                                                                                      max_features=40000,\n",
       "                                                                                                      preprocessor=<function my_preprocessor at 0x7fafdb2c93a0>,\n",
       "                                                                                                      stop_words='english'),\n",
       "                                                                                  'title'),\n",
       "                                                                                 ('mm',\n",
       "                                                                                  MinMaxScaler(),\n",
       "                                                                                  ['num_comments']),\n",
       "                                                                                 ('ohe',\n",
       "                                                                                  OneHotEncoder(drop='first',\n",
       "                                                                                                sparse_output=False),\n",
       "                                                                                  ['domain'])])),\n",
       "                                                ('nb', MultinomialNB())]))],\n",
       "                   final_estimator=LogisticRegression())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_model_subs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92833c05-1d81-486d-a7bc-437e5e0cdd69",
   "metadata": {},
   "source": [
    "Although the stacked model in itself may not be that interpretable, the individual models that make up our StackingClassifier all have some degree of interpretability:\n",
    "1) for Logistic Regression, each feature has coefficients that return the log odds of a certain feature changing the odds towards or away from class 1\n",
    "2) Random Forest returns a list of feature importances\n",
    "3) Multinomial NB returns a list of log probabilities for each class, that that feature is part of an entry of that class <br>\n",
    "\n",
    "Which of the models was weighed the most by the final estimator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "689220da-f1a4-41a8-a277-d8e7cb491c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression weight: 0.746\n",
      "Random Forest weight: 5.662\n",
      "Naive Bayes weight: 1.94\n"
     ]
    }
   ],
   "source": [
    "models = ['Logistic Regression', 'Random Forest', 'Naive Bayes']\n",
    "final_coefs = stacked_model_subs.final_estimator_.coef_[0]\n",
    "for i in range(3):\n",
    "    print(f'{models[i]} weight: {round(final_coefs[i],3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9421eb-62ad-4b8b-9bf9-d9ddeb9a7e2b",
   "metadata": {},
   "source": [
    "### 1.1) Random Forest interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bd2e46-3b54-487d-90d3-1fec17043a08",
   "metadata": {},
   "source": [
    "Above, we can see that the model that is weighted the most in the stacked model is the Random Forest classifier. For that specific model, we can get a list of feature importances to figure out which features the model thought were the most important. In order to access the attributes of the Random Forest Classifier, we need to call the named_estimators_ attribute of the stacked model and select model 2. As each base model is a Pipeline object in itself, we then need to access the random forest with the named_steps attribute and use the alias we specified for the random forest (rf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daefb61b-8bf0-4eb2-a362-600dd7b56537",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_in_stack = stacked_model_subs.named_estimators_.m2.named_steps.rf\n",
    "rf_importances = rf_in_stack.feature_importances_\n",
    "ct2_in_stack = stacked_model_subs.named_estimators_.m2.named_steps.ct\n",
    "rf_feature_names = ct2_in_stack.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "985eb957-7b9f-4da0-acb5-ec9fdf2d5eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_f = zip(rf_feature_names, rf_importances)\n",
    "sorted_f = sorted(zipped_f, key= lambda x: x[1], reverse=True)[:500]\n",
    "\n",
    "hi_mono_names, hi_bi_names = [], []\n",
    "hi_mono_imp, hi_bi_imp = [], []\n",
    "\n",
    "for feature in sorted_f:\n",
    "    if 'tfbg' in feature[0]:\n",
    "        if len(feature[0].split(' ')) == 1:\n",
    "            hi_mono_names.append(feature[0][6:])\n",
    "            hi_mono_imp.append(feature[1])\n",
    "        if len(feature[0].split(' ')) == 2:\n",
    "            hi_bi_names.append(feature[0][6:])\n",
    "            hi_bi_imp.append(feature[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db4d476-82b3-493d-9b48-cebd356f7622",
   "metadata": {},
   "source": [
    "According to the random forest classifier, The additional features 'domain' and num_comments' help the most with classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4cb8048-ee2c-4f52-85a4-bc5fb4636cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ss__num_comments', 0.13504547947870602),\n",
       " ('ohe__domain_reddit', 0.021788010391990447),\n",
       " ('ohe__domain_image', 0.01865944191020541),\n",
       " ('tfbg__trump', 0.01256331212122673),\n",
       " ('ohe__domain_breitbart', 0.011811011210621633),\n",
       " ('ohe__domain_foxnews', 0.010183111754516814),\n",
       " ('ohe__domain_tampafp', 0.009680137580473275),\n",
       " ('ohe__domain_none', 0.00958368723814532),\n",
       " ('ohe__domain_nypost', 0.009103336976758216),\n",
       " ('ohe__domain_washingtonpost', 0.008131078008063192),\n",
       " ('ohe__domain_cnn', 0.007861549479325752),\n",
       " ('ohe__domain_thehill', 0.0074885574957576),\n",
       " ('ohe__domain_nytimes', 0.007053461086324851),\n",
       " ('ohe__domain_redstate', 0.0070488011878285005),\n",
       " ('ohe__domain_other', 0.006803747182445233),\n",
       " ('ohe__domain_politico', 0.005494617110135774),\n",
       " ('tfbg__biden', 0.0045070407840302334),\n",
       " ('ohe__domain_huffpost', 0.0042669915513446335),\n",
       " ('ohe__domain_youtube', 0.004218938333132806),\n",
       " ('ohe__domain_theguardian', 0.004169192498097183)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_f[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12050b3d-78d4-4a37-a09c-479f76dcedc2",
   "metadata": {},
   "source": [
    "One thing that is not directly obvious from these feature importances is the <i>direction</i>. In other words, does the word 'trump' being mentioned more often imply that the post is more likely to come from r/politics, or from r/conservative?\n",
    "One way to figure out the direction would be to simply calculate the pearson r coefficient between the feature and the subreddit. I got that idea from stackoverflow, here:\n",
    "https://stackoverflow.com/questions/58314707/how-to-distinguish-the-direction-of-important-features-from-xgboost-or-random-fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59d9be11-e246-4bb7-83b4-af5fbf866f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pearson_r_feature(feature_list, X_trans, y_train):\n",
    "    \"\"\"\n",
    "    Calculates the pearson r correlation coef from all tfbg columns passed in feature_list and the y data, given the X and y data passed to the classifier \n",
    "    \"\"\"\n",
    "    dir_list = [] #list with feature direction\n",
    "    for feature in feature_list:\n",
    "        ind = np.where('tfbg__' + feature == rf_feature_names)[0][0]\n",
    "        yl = y_train.shape[0]\n",
    "        X_col = X_trans[:,ind].todense().reshape(yl)\n",
    "        p_r = np.corrcoef(X_col, y_train)[0,1]\n",
    "        if p_r <= 0: \n",
    "            #A negative coefficient means more towards zero (= r/conservative)\n",
    "            dir_list.append(0)\n",
    "        elif p_r > 0:\n",
    "            dir_list.append(1)\n",
    "    return dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "408618c2-b921-45fc-9cdc-d7a06f0c2d07",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tfidf_BigramReducer' object has no attribute 'bf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:825\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ready_batches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;66;03m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/queue.py:167\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#transform the training data again to access tfidf columns\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_tr \u001b[38;5;241m=\u001b[39m \u001b[43mct2_in_stack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dir_list_mono \u001b[38;5;241m=\u001b[39m calc_pearson_r_feature(hi_mono_names, X_tr, y_train)\n\u001b[1;32m      4\u001b[0m dir_list_bi \u001b[38;5;241m=\u001b[39m calc_pearson_r_feature(hi_bi_names, X_tr, y_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:727\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m--> 727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:658\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[0;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[1;32m    652\u001b[0m transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(\n\u001b[1;32m    654\u001b[0m         fitted\u001b[38;5;241m=\u001b[39mfitted, replace_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, column_as_strings\u001b[38;5;241m=\u001b[39mcolumn_as_strings\n\u001b[1;32m    655\u001b[0m     )\n\u001b[1;32m    656\u001b[0m )\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfitted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mColumnTransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:1048\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py:836\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_effective_n_jobs\n\u001b[1;32m    834\u001b[0m big_batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m n_jobs\n\u001b[0;32m--> 836\u001b[0m islice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(islice) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/parallel.py:59\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# in a different thread depending on the backend and on the value of\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# pre_dispatch and n_jobs.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m---> 59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/compose/_column_transformer.py:660\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    652\u001b[0m transformers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(\n\u001b[1;32m    654\u001b[0m         fitted\u001b[38;5;241m=\u001b[39mfitted, replace_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, column_as_strings\u001b[38;5;241m=\u001b[39mcolumn_as_strings\n\u001b[1;32m    655\u001b[0m     )\n\u001b[1;32m    656\u001b[0m )\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[1;32m    659\u001b[0m         delayed(func)(\n\u001b[0;32m--> 660\u001b[0m             transformer\u001b[38;5;241m=\u001b[39m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[1;32m    661\u001b[0m             X\u001b[38;5;241m=\u001b[39m_safe_indexing(X, column, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    662\u001b[0m             y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    663\u001b[0m             weight\u001b[38;5;241m=\u001b[39mweight,\n\u001b[1;32m    664\u001b[0m             message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    665\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(name, idx, \u001b[38;5;28mlen\u001b[39m(transformers)),\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, trans, column, weight) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transformers, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    668\u001b[0m     )\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:91\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     89\u001b[0m     new_object_params[name] \u001b[38;5;241m=\u001b[39m clone(param, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     90\u001b[0m new_object \u001b[38;5;241m=\u001b[39m klass(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_object_params)\n\u001b[0;32m---> 91\u001b[0m params_set \u001b[38;5;241m=\u001b[39m \u001b[43mnew_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# quick sanity check of the parameters of the clone\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m new_object_params:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py:170\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    168\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names():\n\u001b[0;32m--> 170\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mtype\u001b[39m):\n\u001b[1;32m    172\u001b[0m         deep_items \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mget_params()\u001b[38;5;241m.\u001b[39mitems()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tfidf_BigramReducer' object has no attribute 'bf'"
     ]
    }
   ],
   "source": [
    "#transform the training data again to access tfidf columns\n",
    "X_tr = ct2_in_stack.fit_transform(X_train)\n",
    "dir_list_mono = calc_pearson_r_feature(hi_mono_names, X_tr, y_train)\n",
    "dir_list_bi = calc_pearson_r_feature(hi_bi_names, X_tr, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce7252-b81a-450c-b3b6-14461a847426",
   "metadata": {},
   "source": [
    "Now let's make a barplot of the monograms and bigrams with the most relative importance. I'll color-code each entry red or blue, depending on whether the occurence of the feature means the post is more likely to be from r/conservative or from r/politics, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ec738-2f5e-4321-9b2a-778388308a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_barplot(ax, wl, values, labels, directions):\n",
    "    \"\"\"\n",
    "    Make a horizontal barplot for the most important random forest features, colorcoding them by direction\n",
    "    \"\"\"\n",
    "    yl = np.arange(wl)\n",
    "    clr = ['tab:red' if x <= 0 else 'tab:blue' for x in directions[:wl]]\n",
    "    ax.barh(yl, values[:wl], color=clr)\n",
    "    ax.set_yticks(yl)\n",
    "    ax.set_yticklabels(labels=labels[:wl], fontsize=12)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Relative weight', fontsize=12)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10accb2b-49ba-4ef1-8594-be4f5219c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 8))\n",
    "\n",
    "make_barplot(ax2, 15, hi_mono_imp, hi_mono_names, dir_list_mono)\n",
    "make_barplot(ax1, 15, hi_bi_imp, hi_bi_names, dir_list_bi)\n",
    "\n",
    "ax1.set_title('Posts: Top 15 most important bigrams in Random Forest', fontweight='bold', fontsize=14)\n",
    "ax2.set_title('Posts: Top 15 most important monograms in Random Forest', fontweight='bold', fontsize=14)\n",
    "fig.savefig('../figures/sub_rf_features.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a690f4-4bf4-4223-aeae-b0fb20044ea1",
   "metadata": {},
   "source": [
    "### 1.2) Logistic Regression Interpretation\n",
    "\n",
    "Let's look at the coefficients of the logistic regression model. We can access the attributes we want in a similar manner as for the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d91b8d-ee76-44f6-8dbf-ee276be08613",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_in_stack = stacked_model_subs.named_estimators_.m1.named_steps.lr\n",
    "lr_coefs = lr_in_stack.coef_[0]\n",
    "ct1_in_stack = stacked_model_subs.named_estimators_.m1.named_steps.ct\n",
    "lr_feature_names = ct1_in_stack.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a1e967-e1d0-4b24-b4f3-545befb09c62",
   "metadata": {},
   "source": [
    "Now let's do a similar thing as for the random forest classifier - we sort the coefficients (by absolute value, we care about high values both positive and negative), and then get the most important tfbg related columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bdb5d1-1e00-4cfc-ba76-920e1c0936dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_lr = zip(lr_feature_names, lr_coefs)\n",
    "sorted_lr = sorted(zipped_lr, key= lambda x: np.abs(x[1]), reverse=True)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d797b-b65d-42f6-ab7e-4e9f54469b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_lr[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b272b1f5-c3e6-4f94-a5c6-f64b59263c94",
   "metadata": {},
   "source": [
    "Again, it's clear the domain features carried a lot of predictive power!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07af6c0-c714-4b8e-ac0c-dcb3ca9c5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_mono_names, hi_bi_names = [], []\n",
    "hi_mono_imp, hi_bi_imp = [], []\n",
    "\n",
    "for feature in sorted_lr:\n",
    "    if 'tfbg' in feature[0]:\n",
    "        if len(feature[0].split(' ')) == 1:\n",
    "            hi_mono_names.append(feature[0][6:])\n",
    "            hi_mono_imp.append(feature[1])\n",
    "        if len(feature[0].split(' ')) == 2:\n",
    "            hi_bi_names.append(feature[0][6:])\n",
    "            hi_bi_imp.append(feature[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2644a6-d728-4f95-96d0-967922577038",
   "metadata": {},
   "source": [
    "We can again make a barplot of the most important monograms and bigrams. It is important to keep in mind that these coefficients measure something "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f6c54-d470-4b4c-91e5-69f513a80a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_barplot_lr(ax, wl, values, labels):\n",
    "    \"\"\"\n",
    "    Make a horizontal barplot for the largest logarithmic regression coefficients\n",
    "    \"\"\"\n",
    "    yl = np.arange(wl)\n",
    "    clr = ['tab:red' if x <= 0 else 'tab:blue' for x in values[:wl]]\n",
    "    ax.barh(yl, values[:wl], color=clr)\n",
    "    ax.set_yticks(yl)\n",
    "    ax.set_yticklabels(labels=labels[:wl], fontsize=12)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Log odds coefficient', fontsize=12)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395232ab-55c4-4cee-a041-390fc811788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 7))\n",
    "\n",
    "make_barplot_lr(ax2, 15, hi_mono_imp, hi_mono_names)\n",
    "make_barplot_lr(ax1, 15, hi_bi_imp, hi_bi_names)\n",
    "\n",
    "ax1.set_title('Posts: 15 bigrams with largest Logreg coefs', fontweight='bold', fontsize=14)\n",
    "ax2.set_title('Posts: 15 monograms with largest Logreg coefs', fontweight='bold', fontsize=14)\n",
    "fig.savefig('../figures/sub_lr_features.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f09c00-d80b-42f6-b93b-507b9ac4eca1",
   "metadata": {},
   "source": [
    "The usage of TfidfVectorizer rather than the more simple CountVectorizer does make the interpretation more difficult here. If it were CountVectorizer, we could simply say that one occurence of the word of the n-gram would increase or decrease the odds according to the measured coefficient. But with TfidfVectorizer, the words are weighted by the inverse document frequency. And so what 'an occurence' means becomes a more abstract, normalized thing. Still, these plots do give us some impression of how certain words will swing the odds towards or away from either of the subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cd42d8-8537-4dae-89fd-df87d776ee52",
   "metadata": {},
   "source": [
    "### 1.3) Naive Bayes interpretation\n",
    "\n",
    "For this algorithm, each feature includes conditional probabilities for each class that the feature is in a given entry of that class. I will divide the e-power of both, to give the relative probability of a given feature being in one class vs the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb135d6-91b9-48b1-a606-2dba89d917e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_in_stack = stacked_model_subs.named_estimators_.m3.named_steps.nb\n",
    "nb_logprobs = nb_in_stack.feature_log_prob_\n",
    "nb_relprobs = np.exp(nb_logprobs[1])/np.exp(nb_logprobs[0])\n",
    "ct3_in_stack = stacked_model_subs.named_estimators_.m3.named_steps.ct\n",
    "nb_feature_names = ct3_in_stack.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50651c-6a24-4dc7-8231-5c61e17bebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_nbf = zip(nb_feature_names, nb_relprobs)\n",
    "sorted_nbf = sorted(zipped_nbf, key= lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b141e68-0b55-414f-be83-699e52eb882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_nbf[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f31c0-606c-4fd8-8aa6-87f29b597975",
   "metadata": {},
   "source": [
    "It's no surprise that the 'domain' features weigh very heavily in this model as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9a858-8349-4887-a9f4-d6fd7e1d2cd9",
   "metadata": {},
   "source": [
    "Now let's make a final bar chart showing the largest relative probabilities for monograms and bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989744c3-721b-4d87-9fb2-9134b8030235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(feature, name_mono, imp_mono, name_bi, imp_bi, inv=False):\n",
    "    \"\"\"\n",
    "    Adds feature name and feature importance to a set of lists, independently for monograms and bigrams\n",
    "    \"\"\"\n",
    "    if 'tfbg' in feature[0]: \n",
    "        if len(feature[0].split(' ')) == 1:\n",
    "            name_mono.append(feature[0][6:])\n",
    "            imp_mono.append(feature[1])\n",
    "        if len(feature[0].split(' ')) == 2:\n",
    "            name_bi.append(feature[0][6:])\n",
    "            imp_bi.append(feature[1])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d9ec2-6a29-445d-becb-3bbca3c414d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_mono_names_p, hi_bi_names_p = [], []\n",
    "hi_mono_imp_p, hi_bi_imp_p = [], []\n",
    "\n",
    "hi_mono_names_c, hi_bi_names_c = [], []\n",
    "hi_mono_imp_c, hi_bi_imp_c = [], []\n",
    "\n",
    "for i in range(200):\n",
    "    feature_p = sorted_nbf[i]\n",
    "    feature_c = sorted_nbf[-(i+1)]\n",
    "    \n",
    "    add_features(feature_p, hi_mono_names_p, hi_mono_imp_p,  hi_bi_names_p, hi_bi_imp_p)\n",
    "    add_features(feature_c, hi_mono_names_c, hi_mono_imp_c,  hi_bi_names_c, hi_bi_imp_c, inv=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2290f1e2-a9dc-44f4-95e1-56c8e528b9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_barplot_nb(ax, wl, val_c, val_p, label_c, label_p):\n",
    "    \"\"\"\n",
    "    Make a horizontal barplot for the largest Naive Bayes relative odds\n",
    "    \"\"\"\n",
    "    yl = np.arange(wl*2)\n",
    "    clr = ['tab:red'] * wl + ['tab:blue'] * wl\n",
    "\n",
    "    val_c = [1/x for x in val_c]\n",
    "    ax.barh(yl, val_c[:wl] + val_p[:wl], color=clr)\n",
    "    ax.set_yticks(yl)\n",
    "    ax.set_yticklabels(labels=label_c[:wl] + label_p[:wl], fontsize=12)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Relative odds', fontsize=12)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf3660d-c7ab-4694-bbbb-fafc3127995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 8))\n",
    "\n",
    "make_barplot_nb(ax2, 8, hi_mono_imp_c, hi_mono_imp_p, hi_mono_names_c, hi_mono_names_p)\n",
    "make_barplot_nb(ax1, 8, hi_bi_imp_c, hi_bi_imp_p, hi_bi_names_c, hi_bi_names_p)\n",
    "\n",
    "ax1.set_title('Posts: Most predictive bigrams in Naive Bayes', fontweight='bold', fontsize=14)\n",
    "ax2.set_title('Posts: Most predictive monograms in Naive Bayes', fontweight='bold', fontsize=14)\n",
    "ax1.set_xlim(0, 9.5)\n",
    "ax2.set_xlim(0, 9.5)\n",
    "fig.savefig('../figures/sub_nb_features.png', dpi=300)\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e8baf-e59f-4cf2-b751-c0cbcc0b3eec",
   "metadata": {},
   "source": [
    "### 1.4) Title model insights: Summary\n",
    "\n",
    "In the cells above, I've gone over the 3 different individual models that make up the stacking classifier, and tried make some visualizations that can help us interpret the importance of certain features. \n",
    "\n",
    "For each of the base models, it is clear that he 'domain' features add a lot of predictive power to the model. The 'domain' features specify which website the link comes from (provided it's a link, which it very often is). This makes a lot of sense on an intuitive level, as the mainstream media and conservative media are often very different. The fact that domain names are so predictive reinforces the idea that the two media universes do not mix a lot at all.\n",
    "\n",
    "The feature importances, coefficients and relative probabilities of the different models in the stacking classifier add a lot of potential for insight into the way news is phrased and what news is discussed in the two different subreddits. \n",
    "\n",
    "Finally, let's repeat what the accuracy of our model is against the baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c66e4-f59f-4ff3-8689-61cae8a7ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b5271-5ec0-4b3c-a23a-134c6ba9e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = stacked_model_subs.score(X_train, y_train)\n",
    "test_acc = stacked_model_subs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3576dd8-4afd-489d-a383-420f78a93095",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The accuracy on the training data is {round(train_acc, 5)}')\n",
    "print(f'The accuracy on the test data is {round(test_acc, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a607aa5-ef86-4767-98aa-e41bbcddf7d9",
   "metadata": {},
   "source": [
    "Although the model is overfit, we still end up with a good performance overall. Against a baseline accuracy of 52.6\\%, we end up with an accuracy of 88.1\\% on the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1d4fb-e4d8-435b-96e0-e570db1beddc",
   "metadata": {},
   "source": [
    "## 2) Comments model: insights\n",
    "\n",
    "I will follow a similar procedure as for the title model. Let's start with again loading in the data and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd3e166-299c-45ab-8ca1-b2b2beae949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_df = pd.read_csv('../data/all_comments_sentiment.csv')\n",
    "com_df.set_index('id', inplace=True)\n",
    "com_df.head()\n",
    "\n",
    "com_df['subreddit'] = com_df['subreddit'].map({'conservative':0, 'politics':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77facc6-45e1-46b0-a7a2-d50095454a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = com_df[['body', 'score', 'word_length', 'freq_poster', 'sent_label']]\n",
    "y = com_df['subreddit']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2293eb81-3db4-4a45-8039-5c10033dbfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickled_models/stacked_model_comments.pkl', 'rb') as pickle_in:\n",
    "    stacked_model_coms = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ee7b9-0b14-45c3-9d69-1ad149ee2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_model_coms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e01857-53b5-4407-b552-e90ef6174a2e",
   "metadata": {},
   "source": [
    "How much were each of the models weighed by the final estimator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eb28dc-58d7-4456-9c2b-d8a436140604",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Logistic Regression', 'Random Forest', 'Naive Bayes']\n",
    "final_coefs = stacked_model_coms.final_estimator_.coef_[0]\n",
    "for i in range(3):\n",
    "    print(f'{models[i]} weight: {round(final_coefs[i],3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fbbf9-d1d7-41f9-9ad2-7dc246b6e5bb",
   "metadata": {},
   "source": [
    "It appears that logistic regression did a lot worse, relatively speaking, for the comments. But both Random Forest and Naive Bayes were weighed relatively high, similar to the Titles model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e94095c-e111-4009-8416-d80ccc846a6c",
   "metadata": {},
   "source": [
    "### 2.1) Random Forest interpretation\n",
    "\n",
    "I can repeat a lot of the steps I took for the titles model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9193bb-eb7f-4c37-9e1c-dd389cec97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_in_stack = stacked_model_coms.named_estimators_.m2.named_steps.rf\n",
    "rf_importances = rf_in_stack.feature_importances_\n",
    "ct2_in_stack = stacked_model_coms.named_estimators_.m2.named_steps.ct\n",
    "rf_feature_names = ct2_in_stack.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619018d7-b3e1-4f3a-856c-36e03a458dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_f = zip(rf_feature_names, rf_importances)\n",
    "sorted_f = sorted(zipped_f, key= lambda x: x[1], reverse=True)[:700]\n",
    "\n",
    "hi_mono_names, hi_bi_names = [], []\n",
    "hi_mono_imp, hi_bi_imp = [], []\n",
    "\n",
    "for feature in sorted_f:\n",
    "    add_features(feature, hi_mono_names, hi_mono_imp,  hi_bi_names, hi_bi_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689def72-6bed-406e-878d-3a7fccfa14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the training data again to access tfidf columns\n",
    "X_tr = ct2_in_stack.fit_transform(X_train)\n",
    "dir_list_mono = calc_pearson_r_feature(hi_mono_names, X_tr, y_train)\n",
    "dir_list_bi = calc_pearson_r_feature(hi_bi_names, X_tr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b762e9-afc1-4520-9db7-98d0380a493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 8))\n",
    "\n",
    "make_barplot(ax2, 15, hi_mono_imp, hi_mono_names, dir_list_mono)\n",
    "make_barplot(ax1, 15, hi_bi_imp, hi_bi_names, dir_list_bi)\n",
    "\n",
    "ax1.set_title('Comments: Top 15 most important bigrams in Random Forest', fontweight='bold', fontsize=14)\n",
    "ax2.set_title('Comments: Top 15 most important monograms in Random Forest', fontweight='bold', fontsize=14)\n",
    "\n",
    "fig.savefig('../figures/com_rf_features.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa77cc-cc61-4043-8498-415a936795e4",
   "metadata": {},
   "source": [
    "### 2.2) Logistic Regression Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f811471-b699-4607-be2e-70e218f8568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_in_stack = stacked_model_coms.named_estimators_.m1.named_steps.lr\n",
    "lr_coefs = lr_in_stack.coef_[0]\n",
    "ct1_in_stack = stacked_model_coms.named_estimators_.m1.named_steps.ct\n",
    "lr_feature_names = ct1_in_stack.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122806d-4920-40ee-a665-9ba37869a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_lr = zip(lr_feature_names, lr_coefs)\n",
    "sorted_lr = sorted(zipped_lr, key= lambda x: np.abs(x[1]), reverse=True)[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fd12f5-0ebe-4afc-8c90-5cff8bd52cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_lr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3af25-85c0-4b2b-9c55-0f00d652e6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_mono_names, hi_bi_names = [], []\n",
    "hi_mono_imp, hi_bi_imp = [], []\n",
    "\n",
    "for feature in sorted_lr:\n",
    "    add_features(feature, hi_mono_names, hi_mono_imp,  hi_bi_names, hi_bi_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559e838-2c4a-4cb1-94c6-0905a250f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 7))\n",
    "\n",
    "make_barplot_lr(ax2, 15, hi_mono_imp, hi_mono_names)\n",
    "make_barplot_lr(ax1, 15, hi_bi_imp, hi_bi_names)\n",
    "\n",
    "ax1.set_title('Comments: 15 bigrams with largest Logreg coefs', fontweight='bold', fontsize=14)\n",
    "ax2.set_title('Comments: 15 monograms with largest Logreg coefs', fontweight='bold', fontsize=14)\n",
    "\n",
    "fig.savefig('../figures/com_lr_features.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873cd51-862a-4710-9c91-2d95b2615ba2",
   "metadata": {},
   "source": [
    "### 2.3) Naive Bayes Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2286b9d-8ac4-4c74-92b2-ecc7cb332960",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_in_stack = stacked_model_coms.named_estimators_.m3.named_steps.nb\n",
    "nb_logprobs = nb_in_stack.feature_log_prob_\n",
    "nb_relprobs = np.exp(nb_logprobs[1])/np.exp(nb_logprobs[0])\n",
    "ct3_in_stack = stacked_model_coms.named_estimators_.m3.named_steps.ct\n",
    "nb_feature_names = ct3_in_stack.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43449b61-af2f-4acf-a450-287262bb89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_nbf = zip(nb_feature_names, nb_relprobs)\n",
    "sorted_nbf = sorted(zipped_nbf, key= lambda x: x[1], reverse=True)\n",
    "\n",
    "sorted_nbf[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab17dd31-abc1-4e5b-9c14-11528cf4b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi_mono_names_p, hi_bi_names_p = [], []\n",
    "hi_mono_imp_p, hi_bi_imp_p = [], []\n",
    "\n",
    "hi_mono_names_c, hi_bi_names_c = [], []\n",
    "hi_mono_imp_c, hi_bi_imp_c = [], []\n",
    "\n",
    "for i in range(200):\n",
    "    feature_p = sorted_nbf[i]\n",
    "    feature_c = sorted_nbf[-(i+1)]\n",
    "    \n",
    "    add_features(feature_p, hi_mono_names_p, hi_mono_imp_p,  hi_bi_names_p, hi_bi_imp_p)\n",
    "    add_features(feature_c, hi_mono_names_c, hi_mono_imp_c,  hi_bi_names_c, hi_bi_imp_c, inv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ed398-b444-421a-9d9d-81182837c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17.5, 8))\n",
    "\n",
    "make_barplot_nb(ax2, 8, hi_mono_imp_c, hi_mono_imp_p, hi_mono_names_c, hi_mono_names_p)\n",
    "make_barplot_nb(ax1, 8, hi_bi_imp_c, hi_bi_imp_p, hi_bi_names_c, hi_bi_names_p)\n",
    "\n",
    "ax1.set_title('Comments: Most predictive bigrams in Naive Bayes', fontweight='bold', fontsize=14)\n",
    "ax2.set_title('Comments: Most predictive monograms in Naive Bayes', fontweight='bold', fontsize=14)\n",
    "\n",
    "fig.savefig('../figures/com_nb_features.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3960f-da17-40f7-94cf-3c1564feca26",
   "metadata": {},
   "source": [
    "### 2.4) Misclassified Comments\n",
    "\n",
    "Given that the overall classification rate of the model wasn't very good - is there anything we can learn from which comments were missclassified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebd1846-5f7c-4ba7-8b1c-52ca0d19b5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = stacked_model_coms.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5393f8-b378-4c42-9945-b33476d492f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_il = np.where(preds_test == y_test)\n",
    "mc_il = np.where(preds_test != y_test)\n",
    "\n",
    "c_idx = y_test.iloc[c_il].index\n",
    "mc_idx = y_test.iloc[mc_il].index\n",
    "\n",
    "com_class = com_df.loc[c_idx,:]\n",
    "com_mclass = com_df.loc[mc_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61afb9a-a96f-4e85-b998-1437c1e1c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_class.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417803ce-8068-4734-a4ba-8b2428d37c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_class.shape, com_mclass.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c2a81-bd03-4d95-babe-7769e52bcd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "logbins = np.logspace(0, 3, 19)\n",
    "ax.hist(com_class['word_length'], histtype='step', linewidth=3, bins=logbins, label='Correctly classified')\n",
    "ax.hist(com_mclass['word_length'], histtype='step', linewidth=3,  bins=logbins, label='Misclassified')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.legend(frameon=False, numpoints=1, fontsize=14)\n",
    "ax.set_xlabel('Comment word length', fontsize=14)\n",
    "ax.set_ylabel('Frequency', fontsize=14)\n",
    "ax.set_title('Word length distribution of comments', fontsize=14, fontweight='bold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844e195-2290-4aaa-854f-37f26dfb56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(com_class[['word_length']].describe().T)\n",
    "display(com_mclass[['word_length']].describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60edb128-9fe8-4f75-8c1a-e21574bd9418",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_lens = np.logspace(0.5, 2.5, 19)\n",
    "\n",
    "cl_up = []\n",
    "for com_len in com_lens:\n",
    "    long_coms_c = com_class[com_class['word_length'] > com_len].shape[0]\n",
    "    long_coms_mc = com_mclass[com_mclass['word_length'] > com_len].shape[0]\n",
    "    len_acc = long_coms_c/(long_coms_c + long_coms_mc)\n",
    "    cl_up.append(len_acc)\n",
    "\n",
    "cl_down =[]\n",
    "for com_len in com_lens:\n",
    "    long_coms_c = com_class[com_class['word_length'] < com_len].shape[0]\n",
    "    long_coms_mc = com_mclass[com_mclass['word_length'] < com_len].shape[0]\n",
    "    len_acc = long_coms_c/(long_coms_c + long_coms_mc)\n",
    "    cl_down.append(len_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f5c6c-2252-4078-8c9b-8f07c9327104",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.plot(com_lens, cl_up, color='salmon', marker='o', linewidth=3, label='Above this word length')\n",
    "ax.plot(com_lens, cl_down, color='grey',  marker='o', linewidth=3, label='Below this word length')\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Word Length', fontsize=14)\n",
    "ax.legend(numpoints=1, frameon=False, fontsize=14)\n",
    "ax.set_title('Cumulative word length vs accuracy',  fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy', fontsize=14)\n",
    "fig.savefig('../figures/wl_acc.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd669f-ef2a-45fd-affd-28b43d8329a5",
   "metadata": {},
   "source": [
    "Misclassified comments are shorter on average, which makes sense - there is less for the model to grab onto. But still, it takes quite a while for the effect to be significant. Even in comments with 100 words, the accuracy is still 'only' 77% - still worse than the Title model.\n",
    "\n",
    "Can we say something about the vectorized content of the misclassified rows? Let's use the ColumnTransformer in the random forest pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d9d52-5e1e-4c12-98e3-90023a00ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_transform = ct2_in_stack.transform(X_test)\n",
    "Xtest_mc = Xtest_transform[mc_il[0], :]\n",
    "Xtest_c = Xtest_transform[c_il[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0aa41-82db-4d36-97bc-791528768108",
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_X_mc = np.sum(Xtest_mc, axis=1)\n",
    "summed_X_c = np.sum(Xtest_c, axis=1)\n",
    "np.mean(summed_X_mc), np.mean(summed_X_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027ebd8-0aae-4a9b-93b1-a04efc6f88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "linbins = np.linspace(0, 20, 40)\n",
    "ax.hist(summed_X_c, histtype='step', linewidth=3, bins=linbins, density=True, label='Correctly classified')\n",
    "ax.hist(summed_X_mc, histtype='step', linewidth=3,  bins=linbins, density=True, label='Misclassified')\n",
    "\n",
    "ax.legend(frameon=False, numpoints=1, fontsize=14)\n",
    "ax.set_xlabel('Summed TF-IDF score', fontsize=14)\n",
    "ax.set_ylabel('Normalized frequency',fontsize=14)\n",
    "ax.set_title('Summed TF-IDF score of comments', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba56369-92e0-48ad-99d7-f5b6e57718fb",
   "metadata": {},
   "source": [
    "These distributions are again pretty similar, although on average the summed value of the Tfidf words is a little higher for correctly classified comments. Which makes sense, it means there is more chance for the model to grab on to some term that might be predictive.\n",
    "\n",
    "What about the predicted probabilities, using predict_proba? Is it possible that the model is more unsure of misclassified comments (eg. the distribution is closer to 0.5), than for correctly classified comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b3665-415b-40c7-b2b2-4662cb0f0646",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = com_class[['body', 'score', 'word_length', 'freq_poster', 'sent_label']]\n",
    "X_mc = com_mclass[['body', 'score', 'word_length', 'freq_poster', 'sent_label']]\n",
    "\n",
    "probs_c = stacked_model_coms.predict_proba(X_c)\n",
    "probs_mc = stacked_model_coms.predict_proba(X_mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee6b63-02ea-40c3-b902-50b04f765ff3",
   "metadata": {},
   "source": [
    "Let's get the baseline accuracy again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59467eba-0d36-4c67-a461-f8510e4ab01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42bb682-5e6e-4a3d-a024-50e59a582d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_conservative = y.value_counts(normalize=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae82867-3cb4-425e-b097-f6d4ce425022",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "ax.hist(probs_c[:,0], histtype='step', linewidth=3, bins=30, density=True, label='Correctly classified')\n",
    "ax.hist(probs_mc[:,0], histtype='step', linewidth=3,  bins=30, density=True, label='Misclassified')\n",
    "\n",
    "ax.axvline(base_conservative, color='grey', linewidth=3, linestyle='dashed', label='Baseline')\n",
    "ax.legend(frameon=False, numpoints=1, fontsize=14, loc='upper left')\n",
    "ax.set_xlabel('P(r/conservative)', fontsize=14)\n",
    "ax.set_ylabel('Normalized frequency', fontsize=14)\n",
    "ax.set_title('Predicted P(r/conservative) in stacked comments model', fontsize=14, fontweight='bold')\n",
    "fig.savefig('../figures/proba_classes.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb221612-06ad-4fa2-aee5-6782a792450b",
   "metadata": {},
   "source": [
    "This was probably expected, but it's still a good thing to see: for the correctly classified comments, the model is more often more certain in its prediction. The misclassified comments are those where the predicted probability is close to 0.5. Out of all the misclassified comments, how many have a predicted probability between 0.4 and 0.6?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a0a897-1c0c-43c6-b212-e59d3487e554",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of misclassified comments {probs_mc.shape[0]}')\n",
    "mc_close = sum([1 for x in probs_mc[:,0] if x > 0.40 and x < 0.60])\n",
    "print(f'Total number of misclassified comments with 0.4 < proba < 0.6: {mc_close}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc34b4-0f1a-4a92-8bd0-9a6c9c4f89de",
   "metadata": {},
   "source": [
    "So roughly half of the misclassified comments have P(r/conservative) between 0.4 and 0.6, which means the model isn't very sure of these predictions to begin with. \n",
    "\n",
    "One other perspective on the above plot is in terms of bias and variance. When the model is quite sure of a prediction, yet that prediction turns out to be wrong, it probably means that it erroneously placed a lot of confidence in certain model features that it encountered in that comment (high variance). For the predictions close to 0.5, it is more likely to mean that nothing encountered in the comment shifted the probability very far away from the baseline accuracy (high bias). In the comments model, we clearly have both to some extent, which also shows up in the accuracy scores: the accuracy score on the training data is 20% higher than on the testing data, and the overall accuracy isn't all that high.\n",
    "\n",
    "Together with the dependence of accuracy on word length, it's clear that the reason why the accuracy score gets stuck at around 65% is due to some comments simply not containing enough information. There aren't enough 'significant' words that can convince the model one way or the other, and so the predicted probability stays fairly close to the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87d126-4264-4abd-a20b-52f3bf661472",
   "metadata": {},
   "source": [
    "### 2.5) Comments Model insights: Summary\n",
    "\n",
    "In the section aove, I've tried to interpret the output of the base estimators contained in my stacked classifier for the reddit comments. For each of the base models in the stacking classifier, I've created visualizations of the monograms and bigrams that appear to be most predictive of which subreddit the comment comes from.\n",
    "\n",
    "In terms of the model evaluation, the model classifies comments with the follow accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60192b-dee8-4a0e-b5fa-01f0ad8ae241",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = stacked_model_coms.score(X_train, y_train)\n",
    "test_acc = stacked_model_coms.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d05ce2-eebd-41f1-b49b-077896cd4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The accuracy on the training data is {round(train_acc, 5)}')\n",
    "print(f'The accuracy on the test data is {round(test_acc, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983b55b-beef-4452-a3b5-63ed4b07d133",
   "metadata": {},
   "source": [
    "Like with the titles model, the model is overfit significantly - but I was unable to find a model that is less overfit that still does better on the testing data. The overall accuracy on the testing data ifs 65.4%.\n",
    "\n",
    "I've looked at which comments typically get misclassified, and there is a clear correlation visible between overall word length and accuracy. This makes sense, as short comments are less likely to have a word in them that helps the model clearly distinguish between the two subreddits. Misclassified comments more often are in the predicted probability range of 40-60%, which implies that the words in the comment do not move the needle all that much from the baseline model - the model is therefore less sure about these comments.\n",
    "\n",
    "Ultimately, the somewhat low accuracy score for this model is disappointing but perhaps not that surprising. There is little data to go on with some of the comments, and given the fact that the two subreddits chosen for this project are both political, it will typically take more than just a few words for the classifier to work well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf2433-c526-48a8-8f06-748e269e5e07",
   "metadata": {},
   "source": [
    "# 3) Overall Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840737dc-205d-4110-9d8e-219acc61d97b",
   "metadata": {},
   "source": [
    "Using Pushshift API, I collected data from two different subreddits, r/politics and r/conservative, from the month leading up to the 2022 midterms (October 2022). I tested out different classification models in order to classify 1) the post titles, and 2) the comments of either subreddit. After data cleaning, I was left with about 19,000 post titles (53/47 split for r/conservative - r/politics, respectively), and 48,000 comments (47/53 split).  In order to model the language, we used a 'bag of words' approach with a TF-IDF (Term Frequency - Inverse Document Frequency) vectorizer.\n",
    "\n",
    "For the titles model, other than the title I included two additional pieces of information: the number of comments on each post, and the domain name that the post linked to. The final model that I used consists of a Stacking Classifier, using Logistic Regression, Random Forest, and Multinomial Naive Bayes as the base estimators. and a Logistic Regression as the final estimator. The final accuracy on the testing data is <b>88.1%</b>. Analysis of the feature importances indicates that the domain names are highly predictive and help improve the accuracy score a lot. This makes a lot of sense given the fact that the US political media system is highly polarized, and certain websites are only read by conservatives while others are highly avoided by conservatives, and vice versa.\n",
    "\n",
    "For the comments model, I included several other pieces of information: the score of the comment, the word length, whether the commenter is a frequent poster or not, and the sentiment of the comment (Negative/neutral/positive, using sentiment analysis with the roBERTa model). The best-performing model was again a Stacking classifier with the same estimator components as for the titles model. The final accuracy on the testing data is <b>65.4%</b>. Unlike with the titles model, it appears that none of the additional non-language features I added improve the accuracy by very much. One of the main reason for misclassification appears to be the fact that many comments are just too short for the model to be able to classify them with much certainty.\n",
    "\n",
    "The base models that make up the Stacking Classifier allow us to look into which features add predictive power to the model, which means we can look at the language that is used and the degree to which this determines whether the post or comment comes from either subreddit. Although a full analysis of language use is beyond the scope of this project, using the visualizations in this notebook we can point to some directions that might warrant further exploration or analysis: \n",
    "\n",
    "1) <b>Abortion</b>: discussions about this topic have greatly flared up since the Dobbs v Jackson decision by the Supreme Court in June 2022. We can see this discussion show up in different ways in the two subreddits: the phrase 'pro life' appears to be highly predictive of r/conservative, which makes sense as this moniker is often used by anti-abortionists to describe themselves. On the r/politics side, the terms 'Supreme Court' and 'abortion' show up as important features\n",
    "\n",
    "2) <b>Presidents</b>: it appears that 'Biden' is much more mentioned on the conservative side (see also 'geriatric joe') while Trump (and related terms like 'mar lago') is more often mentioned on r/politics \n",
    "\n",
    "3) <b>The pandemic</b>: it appears that on r/politics, COVID and the pandemic may not have been as important in the discourse as on r/conservative. On r/conservative, we can see terms like 'covid' and 'vaccine' as important features, but equivalents are not directly obvious on r/politics\n",
    "\n",
    "4) <b>The other side of the political aisle</b>: both side use certain terms to describe the other side. On r/conservative, we see highly predictive features such as 'left', 'leftists', while r/politics uses terms like 'right wing' and 'far right'.\n",
    "\n",
    "Ultimately, we were quite a bit more succesful in classifying posts compared to comments. This is largely due to the domain feature that is included with the posts model. In this particular case, more comments data might help combat overfitting and avoid the model erroneously adding significance to certain words. Performance may be slightly improved by  more hyperparameter tuning of the stacking classifier as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
